{"cells":[{"cell_type":"code","execution_count":1,"metadata":{"executionInfo":{"elapsed":887,"status":"ok","timestamp":1710735391170,"user":{"displayName":"이Gilbert","userId":"09521568956774973083"},"user_tz":-540},"id":"vTuGxUFu43W2"},"outputs":[],"source":["from google.colab.patches import cv2_imshow"]},{"cell_type":"markdown","metadata":{"id":"nP8hUx5dOEWG"},"source":["# **이미지 다운로드**"]},{"cell_type":"code","execution_count":2,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":3079,"status":"ok","timestamp":1710735394242,"user":{"displayName":"이Gilbert","userId":"09521568956774973083"},"user_tz":-540},"id":"b3kQGYHiqsX8","outputId":"b9d71ddb-b682-4177-d55d-50fffbb1dd4d"},"outputs":[{"name":"stdout","output_type":"stream","text":["--2024-03-18 04:16:30--  https://github.com/leetae04kr/file_to_use/raw/main/cv2/1665315749956.jpg\n","Resolving github.com (github.com)... 140.82.113.4\n","Connecting to github.com (github.com)|140.82.113.4|:443... connected.\n","HTTP request sent, awaiting response... 302 Found\n","Location: https://raw.githubusercontent.com/leetae04kr/file_to_use/main/cv2/1665315749956.jpg [following]\n","--2024-03-18 04:16:31--  https://raw.githubusercontent.com/leetae04kr/file_to_use/main/cv2/1665315749956.jpg\n","Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.110.133, 185.199.109.133, ...\n","Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n","HTTP request sent, awaiting response... 200 OK\n","Length: 4359534 (4.2M) [image/jpeg]\n","Saving to: ‘1665315749956.jpg’\n","\n","1665315749956.jpg   100%[===================\u003e]   4.16M  --.-KB/s    in 0.07s   \n","\n","2024-03-18 04:16:31 (57.5 MB/s) - ‘1665315749956.jpg’ saved [4359534/4359534]\n","\n","--2024-03-18 04:16:31--  https://github.com/leetae04kr/file_to_use/raw/main/cv2/1665315749921.jpg\n","Resolving github.com (github.com)... 140.82.112.3\n","Connecting to github.com (github.com)|140.82.112.3|:443... connected.\n","HTTP request sent, awaiting response... 302 Found\n","Location: https://raw.githubusercontent.com/leetae04kr/file_to_use/main/cv2/1665315749921.jpg [following]\n","--2024-03-18 04:16:31--  https://raw.githubusercontent.com/leetae04kr/file_to_use/main/cv2/1665315749921.jpg\n","Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n","Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n","HTTP request sent, awaiting response... 200 OK\n","Length: 5766190 (5.5M) [application/octet-stream]\n","Saving to: ‘1665315749921.jpg’\n","\n","1665315749921.jpg   100%[===================\u003e]   5.50M  --.-KB/s    in 0.1s    \n","\n","2024-03-18 04:16:32 (53.0 MB/s) - ‘1665315749921.jpg’ saved [5766190/5766190]\n","\n","--2024-03-18 04:16:32--  https://github.com/leetae04kr/file_to_use/raw/main/cv2/20170218_144054.jpg\n","Resolving github.com (github.com)... 140.82.113.4\n","Connecting to github.com (github.com)|140.82.113.4|:443... connected.\n","HTTP request sent, awaiting response... 302 Found\n","Location: https://raw.githubusercontent.com/leetae04kr/file_to_use/main/cv2/20170218_144054.jpg [following]\n","--2024-03-18 04:16:32--  https://raw.githubusercontent.com/leetae04kr/file_to_use/main/cv2/20170218_144054.jpg\n","Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n","Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n","HTTP request sent, awaiting response... 200 OK\n","Length: 1478552 (1.4M) [image/jpeg]\n","Saving to: ‘20170218_144054.jpg’\n","\n","20170218_144054.jpg 100%[===================\u003e]   1.41M  --.-KB/s    in 0.05s   \n","\n","2024-03-18 04:16:33 (26.2 MB/s) - ‘20170218_144054.jpg’ saved [1478552/1478552]\n","\n","--2024-03-18 04:16:33--  https://076923.github.io/assets/posts/Python/OpenCV/lecture-25/1.webp\n","Resolving 076923.github.io (076923.github.io)... 185.199.108.153, 185.199.109.153, 185.199.110.153, ...\n","Connecting to 076923.github.io (076923.github.io)|185.199.108.153|:443... connected.\n","HTTP request sent, awaiting response... 200 OK\n","Length: 5622 (5.5K) [image/webp]\n","Saving to: ‘1.webp’\n","\n","1.webp              100%[===================\u003e]   5.49K  --.-KB/s    in 0s      \n","\n","2024-03-18 04:16:33 (26.7 MB/s) - ‘1.webp’ saved [5622/5622]\n","\n","--2024-03-18 04:16:33--  https://076923.github.io/assets/posts/Python/OpenCV/lecture-37/1.webp\n","Resolving 076923.github.io (076923.github.io)... 185.199.108.153, 185.199.109.153, 185.199.110.153, ...\n","Connecting to 076923.github.io (076923.github.io)|185.199.108.153|:443... connected.\n","HTTP request sent, awaiting response... 200 OK\n","Length: 468036 (457K) [image/webp]\n","Saving to: ‘1.webp.1’\n","\n","1.webp.1            100%[===================\u003e] 457.07K  --.-KB/s    in 0.04s   \n","\n","2024-03-18 04:16:33 (12.5 MB/s) - ‘1.webp.1’ saved [468036/468036]\n","\n","--2024-03-18 04:16:33--  https://076923.github.io/assets/posts/Python/OpenCV/lecture-37/2.webp\n","Resolving 076923.github.io (076923.github.io)... 185.199.108.153, 185.199.109.153, 185.199.110.153, ...\n","Connecting to 076923.github.io (076923.github.io)|185.199.108.153|:443... connected.\n","HTTP request sent, awaiting response... 200 OK\n","Length: 42944 (42K) [image/webp]\n","Saving to: ‘2.webp’\n","\n","2.webp              100%[===================\u003e]  41.94K  --.-KB/s    in 0.01s   \n","\n","2024-03-18 04:16:33 (3.77 MB/s) - ‘2.webp’ saved [42944/42944]\n","\n"]}],"source":["!wget https://github.com/leetae04kr/file_to_use/raw/main/cv2/1665315749956.jpg\n","!wget https://github.com/leetae04kr/file_to_use/raw/main/cv2/1665315749921.jpg\n","!wget https://github.com/leetae04kr/file_to_use/raw/main/cv2/20170218_144054.jpg\n","!wget https://076923.github.io/assets/posts/Python/OpenCV/lecture-25/1.webp\n","!wget https://076923.github.io/assets/posts/Python/OpenCV/lecture-37/1.webp\n","!wget https://076923.github.io/assets/posts/Python/OpenCV/lecture-37/2.webp"]},{"cell_type":"code","execution_count":2,"metadata":{"executionInfo":{"elapsed":4,"status":"ok","timestamp":1710735394243,"user":{"displayName":"이Gilbert","userId":"09521568956774973083"},"user_tz":-540},"id":"QiHUmNLcldl1"},"outputs":[],"source":[]},{"cell_type":"code","execution_count":3,"metadata":{"executionInfo":{"elapsed":3,"status":"ok","timestamp":1710735394243,"user":{"displayName":"이Gilbert","userId":"09521568956774973083"},"user_tz":-540},"id":"U-bRZ8vzrbCJ"},"outputs":[],"source":["img1='/content/1665315749956.jpg'\n","img2='/content/1665315749921.jpg'\n","img3='/content/20170218_144054.jpg'\n","img4='/content/1.webp'\n","img5='/content/1.webp.1'\n","img6='/content/2.webp'"]},{"cell_type":"markdown","metadata":{"id":"THVus9XTrSNU"},"source":["img1\n","\u003cimg src='https://github.com/leetae04kr/file_to_use/raw/main/cv2/1665315749956.jpg'\u003e"]},{"cell_type":"markdown","metadata":{"id":"bvEyEZzBrktX"},"source":["img2\n","\u003cimg src='https://github.com/leetae04kr/file_to_use/raw/main/cv2/1665315749921.jpg'\u003e"]},{"cell_type":"markdown","metadata":{"id":"CkC1NcU7rvGt"},"source":["img3\n","\u003cimg src=' https://github.com/leetae04kr/file_to_use/raw/main/cv2/20170218_144054.jpg'\u003e"]},{"cell_type":"markdown","metadata":{"id":"3EvNaLqFr4dx"},"source":["img4\n","\n","\u003cimg src='https://076923.github.io/assets/posts/Python/OpenCV/lecture-25/1.webp'\u003e\n","\n","img5\n","\u003cimg src='https://076923.github.io/assets/posts/Python/OpenCV/lecture-37/1.webp'\u003e\n","\n","img6\n","\u003cimg src='https://076923.github.io/assets/posts/Python/OpenCV/lecture-37/2.webp'\u003e"]},{"cell_type":"markdown","metadata":{"id":"fcI9kl-KpCti"},"source":["# 출력\n"]},{"cell_type":"markdown","metadata":{"id":"e1tX_8M2XUKv"},"source":["##이미지 출력\n","\n","OpenCV는 래스터 그래픽스 이미지 파일 포맷을 쉽게 불러올 수 있는 별도의 함수를 제공합니다.\n","\n","이 함수는 불러온 압축 해제된 이미지 데이터 구조에 필요한 메모리 할당과 같은 복잡한 작업을 처리하며, 파일 시그니처(File Signature)를 읽어 적절한 코덱을 결정합니다.\n","\n","OpenCV에서 이미지를 불러올 때는 확장자를 확인하는 방식이 아닌 파일 시그니처를 읽어 파일의 포맷을 분석합니다.\n","\n","파일 시그니처는 파일 매직 넘버(File Magic Number)라고도 하며, 각 파일 형식마다 몇 개의 바이트가 지정되어 있습니다.\n","\n","예를 들어, PNG 확장자의 경우 89 50 4E 47 … 형태로 파일 헤더에 포함되어 있습니다.\n","\n","이미지 입력 함수는 운영체제의 코덱을 사용해 운영체제 별로 픽셀값이 다를 수 있습니다."]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true,"output_embedded_package_id":"1d-bmuDXI0jnp0a6rJKWdcPTJqu3Lwrjk"},"id":"3RlynKNcUNMN","outputId":"a43fda34-8baa-4b5b-a6a3-98d8a134a36d"},"outputs":[],"source":["import cv2\n","\n","image = cv2.imread(img1, cv2.IMREAD_ANYCOLOR)\n","cv2_imshow(image)"]},{"cell_type":"markdown","metadata":{"id":"Oy7WCpEiYsPe"},"source":["## 대칭 (Flip, Symmetry)\n","\n","대칭(Flip)은 기하학적인 측면에서 반사(reflection)의 의미를 갖습니다.\n","\n","2차원 유클리드 공간에서의 기하학적인 변환의 하나로\n","R\n","2\n","(2차원 유클리드 공간) 위의 선형 변환을 진행합니다.\n","\n","대칭은 변환할 행렬(이미지)에 대해 2×2 행렬을 왼쪽 곱셈을 진행합니다. 즉, ‘p’ 형태의 물체에 Y축 대칭을 적용한다면 ‘q’ 형태를 갖게 됩니다.\n","\n","그러므로, 원본 행렬(이미지)에 각 축에 대한 대칭을 적용했을 때, 단순히 원본 행렬에서 축에 따라 재매핑을 적용하면 대칭된 행렬을 얻을 수 있습니다.\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true,"output_embedded_package_id":"1IVYcS33r8oYU5wB1ZwenIKNGIVAY7m6A"},"id":"_Sy2Wfo7Yuuv","outputId":"ee18b1fd-c1e5-4242-bd7b-66c95a0c92b1"},"outputs":[],"source":["import cv2\n","\n","src = cv2.imread(img1, cv2.IMREAD_COLOR)\n","dst = cv2.flip(src, 0)\n","\n","cv2_imshow(src)\n","cv2_imshow(dst)\n","cv2.waitKey()\n","cv2.destroyAllWindows()"]},{"cell_type":"markdown","metadata":{"id":"4XO3fVsaY-WO"},"source":["##회전\n","\n","회전(Rotate)은 선형 변환 중 하나에 포함되며, 회전 변환 행렬(Rotation matrix)을 통해 변환이 진행됩니다.\n","\n","회전 변환 행렬은 임의의 점을 중심으로 물체를 회전시킵니다. 회전 변환 행렬의 일부는 반사 행렬(Reflection matrix)과 같은 값을 지닐 수 있습니다.\n","\n","2차원 유클리드 공간에서의 회전은 크게 두 가지 회전 행렬을 갖습니다. 좌푯값을 회전시키는 회전 행렬과 좌표 축을 회전시키는 회전 행렬이 있습니다.\n","\n","좌표 회전 행렬은 원점을 중심으로 좌푯값을 회전시켜 매핑하며, 좌표 축 회전 행렬은 원점을 중심으로 행렬 자체를 회전시켜 새로운 행렬의 값을 구성합니다.\n","\n","OpenCV의 회전 함수는 좌표 축의 회전 이동 행렬과 동일한 형태이며, 비율을 조정하거나 중심점의 기준을 변경하여 회전할 수 있습니다.\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true,"output_embedded_package_id":"1vP6kWzjmCfRxge8fav430N7_t_MhCpuU"},"id":"gOgjAH_0ZKl1","outputId":"747cada6-305d-4577-afe1-6f948b196bd9"},"outputs":[],"source":["import cv2\n","\n","src = cv2.imread(img1, cv2.IMREAD_COLOR)\n","\n","height, width, channel = src.shape\n","matrix = cv2.getRotationMatrix2D((width/2, height/2), 90, 1)\n","dst = cv2.warpAffine(src, matrix, (width, height))\n","\n","cv2_imshow(src)\n","print(\"\"\"\n","\n","\n","\n","\n","\n","\"\"\")\n","cv2_imshow(dst)\n","cv2.waitKey()\n","cv2.destroyAllWindows()"]},{"cell_type":"markdown","metadata":{"id":"wWyAngVmZ4n9"},"source":["##확대 \u0026 축소\n","\n","**이미지 피라미드(Image Pyramid)Permalink**\n","입력 이미지는 항상 동일한 크기가 아니며 너무 작거나 너무 클 수도 있습니다.\n","\n","만약 알고리즘에서 요구하는 해상도가 있다면 입력 이미지의 크기를 변경해 영상 처리를 진행해야 합니다.\n","\n","또한, 검출하려는 객체가 너무 작거나 입력 이미지가 너무 큰 경우 입력 이미지 자체를 변환해서 영상 처리를 진행할 수도 있습니다.\n","\n","이미지 확대와 축소는 이미지 피라미드(Image pyramid)를 활용해 이미지의 크기를 원하는 단계까지 샘플링하는 작업입니다.\n","\n","이미지 피라미드의 의미는 이미지의 크기를 확대하거나 축소했을 때 이미지들의 형태가 피라미드와 같이 표현됩니다.\n","\n","원본 이미지에서 크기를 확대하는 것을 업 샘플링이라 하며 하위 단계의 이미지를 생성하게 됩니다.\n","\n","반대로 원본 이미지에서 크기를 축소하는 것을 다운 샘플링이라 하며, 상위 단계의 이미지를 생성하게 됩니다.\n","\n","이미지 피라미드로는 가우시안 피라미드(Gaussian Pyramid)와 라플라시안 피라미드(Laplacian pyramid)를 활용합니다."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"AzgenhoxbBXJ"},"outputs":[],"source":["import cv2\n","\n","src = cv2.imread(img1, cv2.IMREAD_COLOR)\n","height, width, channel = src.shape\n","\n","dst = cv2.pyrUp(src, dstsize=(width * 2, height * 2), borderType=cv2.BORDER_DEFAULT)\n","dst2 = cv2.pyrDown(src)\n","\n","cv2_imshow(src)\n","cv2_imshow(dst)\n","cv2_imshow(dst2)\n","cv2.waitKey()\n","cv2.destroyAllWindows()"]},{"cell_type":"markdown","metadata":{"id":"p4TCoTuafHHF"},"source":["##크기 조절(Resize)\n","영상이나 이미지의 이미지를 확대하거나 축소하는 연산에서 확인할 수 있듯이 이미지의 크기를 변형하는 것은 단순한 연산이 아닙니다.\n","\n","이미지를 확대하는 경우에는 픽셀에 대한 보간법, 이미지를 축소하는 경우에는 픽셀에 대한 병합법이 수행됩니다.\n","\n","이미지 피라미드는 2배로 확대하거나 축소하는 경우만 가능하므로, 원하는 크기로 변환하기 위해서 이미지 크기 조절 함수를 사용합니다.\n","\n","이미지 크기를 조절하는 방법은 크게 두 가지 방법이 있습니다.\n","\n","첫 번째 방법은 이미지의 크기를 사용자가 요구하는 절대 크기로 변경하는 방법입니다. 즉, 임의의 크기(640×480이나 123×456 등의 이미지 크기)로 변환하는 것을 의미합니다.\n","\n","두 번째 방법은 이미지의 크기를 비율에 맞게 상대 크기로 변경하는 방법입니다. 이 경우, 입력 이미지의 크기와 비례하도록 너비와 높이가 계산됩니다.\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true,"output_embedded_package_id":"1Gp2D1HBupNzZaF6exSBzMos-wwyN99lG"},"id":"bmv61vfLfLs3","outputId":"c1a5f703-f163-4096-b432-0e8aa147dfd5"},"outputs":[],"source":["import cv2\n","\n","src = cv2.imread(img1, cv2.IMREAD_COLOR)\n","\n","dst = cv2.resize(src, dsize=(640, 480), interpolation=cv2.INTER_AREA)\n","dst2 = cv2.resize(src, dsize=(0, 0), fx=0.3, fy=0.7, interpolation=cv2.INTER_LINEAR)\n","\n","cv2_imshow(src)\n","cv2_imshow(dst)\n","cv2_imshow(dst2)\n","cv2.waitKey()\n","cv2.destroyAllWindows()"]},{"cell_type":"markdown","metadata":{"id":"_zBNkrk9f3qN"},"source":["##자르기(Slice)\n","자르기(Slice)는 영상이나 이미지에서 특정 영역을 잘라내는 연산을 의미하니다.\n","\n","특정 영역을 잘라내는 것을 관심 영역(Region Of Interest, ROI)이라 하며, 이미지 상에서 관심 있는 영역을 의미합니다.\n","\n","이미지를 처리할 때 객체를 탐지하거나 검출하는 영역을 명확하게 관심 영역이라 볼 수 있습니다.\n","\n","관심 영역에만 알고리즘을 적용한다면, 불필요한 연산이 줄어들고 정확도가 늘어나는 효과를 얻을 수 있습니다."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"w7-Ey2OUf7v1"},"outputs":[],"source":["import cv2\n","\n","src = cv2.imread(img1, cv2.IMREAD_COLOR)\n","dst = src[100:600, 200:700].copy()\n","\n","cv2_imshow(src)\n","cv2_imshow(dst)\n","cv2.waitKey()\n","cv2.destroyAllWindows()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"2morvRXjgPza"},"outputs":[],"source":["import cv2\n","\n","src = cv2.imread(img1, cv2.IMREAD_COLOR)\n","\n","dst = src.copy()\n","roi = src[100:600, 200:700]\n","dst[0:500, 0:500] = roi\n","\n","cv2_imshow(src)\n","cv2_imshow(dst)\n","cv2.waitKey()\n","cv2.destroyAllWindows()"]},{"cell_type":"markdown","metadata":{"id":"8E3PQubspPye"},"source":["# 필터"]},{"cell_type":"markdown","metadata":{"id":"0mx6JFikgd5S"},"source":["##색상 공간 변환(Convert Color)\n","색상 공간 변환(Convert Color)은 본래의 색상 공간에서 다른 색상 공간으로 변환할 때 사용합니다.\n","\n","색상 공간 변환 함수는 데이터 타입을 같게 유지하고 채널을 변환합니다.\n","\n","입력된 이미지는 8 비트, 16 비트, 32 비트의 정밀도를 갖는 배열을 사용할 수 있습니다.\n","\n","출력된 이미지는 입력된 이미지의 이미지 크기와 정밀도가 동일한 배열이 됩니다.\n","\n","채널의 수가 감소하게 되어 이미지 내부의 데이터는 설정한 색상 공간과 일치하는 값으로 변환되며, 데이터 값이 변경되거나 채널 순서가 변경될 수 있습니다.\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"GJB0sYhKghJ9"},"outputs":[],"source":["import cv2\n","\n","src = cv2.imread(img2, cv2.IMREAD_COLOR)\n","dst = cv2.cvtColor(src, cv2.COLOR_BGR2GRAY)\n","\n","cv2_imshow(src)\n","cv2_imshow(dst)\n","cv2.waitKey()\n","cv2.destroyAllWindows()"]},{"cell_type":"markdown","metadata":{"id":"c4G5TYQXmO7z"},"source":["##역상(Reverse Image)\n","\n","역상(Reverse Image)은 영상이나 이미지를 반전 된 색상으로 변환하기 위해서 사용합니다.\n","\n","픽셀 단위마다 비트 연산(Bitwise Operation)을 적용하는데, 그중 NOT 연산을 적용합니다.\n","\n","NOT 연산은 각 자릿수의 값을 반대로 바꾸는 연산입니다.\n","\n","만약 153의 값을 갖는 픽셀에 NOT 연산을 적용한다면 102의 값으로 변경됩니다.\n","\n","153은 0b10011001의 값을 가지며, 102는 0b01100110의 값을 갖습니다.\n","\n","즉, 10 진수의 픽셀값을 2 진수의 값으로 변경한 다음, 각 자릿수의 값을 반대로 바꾸게 됩니다.\n","\n","1은 0이 되며, 0은 1로 변경됩니다."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"3BjhwKXSmSbW"},"outputs":[],"source":["import cv2\n","\n","src = cv2.imread(img2, cv2.IMREAD_COLOR)\n","dst = cv2.bitwise_not(src)\n","\n","cv2_imshow(src)\n","cv2_imshow(dst)\n","cv2.waitKey()\n","cv2.destroyAllWindows()"]},{"cell_type":"markdown","metadata":{"id":"m1sDiOOLmbfN"},"source":["## 이진화(Binary)\n","이진화(Binary)는 어느 지점을 기준으로 값이 높거나 낮은 픽셀의 값을 대상으로 특정 연산을 수행할 때 사용합니다.\n","\n","일반적으로 값이 높거나 낮은 픽셀을 검은색 또는 흰색의 값으로 변경합니다.\n","\n","기준값에 따라 이분법적으로 구분해 픽셀을 참 또는 거짓으로 나누는 연산이며, 이미지 행렬에서 모든 픽셀에 대해 연산이 수행됩니다.\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"i2pCg5hnmfJk"},"outputs":[],"source":["import cv2\n","\n","src = cv2.imread(img2, cv2.IMREAD_COLOR)\n","\n","gray = cv2.cvtColor(src, cv2.COLOR_BGR2GRAY)\n","ret, dst = cv2.threshold(gray, 100, 255, cv2.THRESH_BINARY)\n","\n","cv2_imshow(dst)\n","cv2.waitKey()\n","cv2.destroyAllWindows()"]},{"cell_type":"markdown","metadata":{"id":"uVdS6USmnOYy"},"source":["## 흐림 효과(Blur)\n","\n","흐림 효과(Blur)는 블러링(Blurring) 또는 스무딩(Smoothing)이라 불리며, 노이즈를 줄이거나 외부 영향을 최소화하는 데 사용됩니다.\n","\n","흐림 효과는 영상이나 이미지를 번지게 하며, 해당 픽셀의 주변 값들과 비교하고 계산해서 픽셀들의 색상을 재조정합니다.\n","\n","단순히 이미지를 흐리게 만드는 것뿐만 아니라 노이즈를 제거해서 연산 시 계산을 빠르고 정확하게 수행하는 데 도움을 줍니다.\n","\n","또한, 이미지의 해상도를 변경하는 경우에도 사용되는데 이미지의 크기를 변경하면 존재하지 않는 데이터를 생성하거나 존재하는 데이터를 줄여야 하므로 샘플링된 이미지를 재구성할 때 사용됩니다.\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"FYAo2GRDnTnD"},"outputs":[],"source":["import cv2\n","\n","src = cv2.imread(img2, cv2.IMREAD_COLOR)\n","dst = cv2.blur(src, (9, 9), anchor=(-1, -1), borderType=cv2.BORDER_DEFAULT)\n","\n","cv2_imshow(dst)\n","cv2.waitKey()\n","cv2.destroyAllWindows()"]},{"cell_type":"markdown","metadata":{"id":"Z6ncaTZQpbr4"},"source":["#컬러검출"]},{"cell_type":"markdown","metadata":{"id":"u1Lozj3boty_"},"source":["## 가장자리 검출(Edge)\n","가장자리(Edge)는 가장 바깥 부분의 둘레를 의미하며, 객체의 테두리로 볼 수 있습니다.\n","\n","이미지 상에서 가장자리는 전경(Foreground)과 배경(Background)이 구분되는 지점이며, 전경과 배경 사이에서 밝기가 큰 폭으로 변하는 지점이 객체의 가장자리가 됩니다.\n","\n","그러므로 가장자리는 픽셀의 밝기가 급격하게 변하는 부분으로 간주할 수 있습니다.\n","\n","가장자리를 찾기 위해 미분(Derivative)과 기울기(Gradient) 연산을 수행하며, 이미지 상에서 픽셀의 밝기 변화율이 높은 경계선을 찾습니다."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"lrimzPvo4deV"},"outputs":[],"source":["import cv2\n","\n","src = cv2.imread(img3, cv2.IMREAD_COLOR)\n","gray = cv2.cvtColor(src, cv2.COLOR_BGR2GRAY)\n","\n","sobel = cv2.Sobel(gray, cv2.CV_8U, 1, 0, 3)\n","laplacian = cv2.Laplacian(gray, cv2.CV_8U, ksize=3)\n","canny = cv2.Canny(src, 100, 255)\n","\n","cv2_imshow(sobel)\n","cv2_imshow(laplacian)\n","cv2_imshow(canny)\n","cv2.waitKey()\n","cv2.destroyAllWindows()"]},{"cell_type":"markdown","metadata":{"id":"u4v4syAf5C4p"},"source":["##HSV(Hue, Saturation, Value)\n","HSV(Hue, Saturation, Value) 공간은 색상을 표현하기에 간편한 색상 공간입니다.\n","\n","이미지에서 색상을 검출한다고 가정할 때 BGR이나 RGB 패턴으로는 인간이 인지하는 영역의 색상을 구별하기에는 매우 어렵고 복잡합니다.\n","\n","하지만 HSV 색상 공간을 활용한다면 간편하고 빠르게 특정 색상을 검출하고 분리할 수 있습니다.\n","\n","색상(Hue)은 빨간색, 노란색, 파란색 등으로 인식되는 색상 중 하나 또는 둘의 조합과 유사한 것처럼 보이는 시각적 감각의 속성을 의미합니다.\n","\n","0°에서 360°의 범위로 표현되며, 파란색은 220°에서 260° 사이에 있습니다. OpenCV에서는 0 ~ 179의 범위로 표현됩니다.\n","\n","채도(Saturation)는 이미지의 색상 깊이로, 색상이 얼마나 선명한(순수한) 색인지를 의미합니다.\n","\n","아무것도 섞지 않아 맑고 깨끗하며 원색에 가까운 것을 채도가 높다고 표현합니다.\n","\n","0%에서 100%의 비율로 표현되며, 0%에 가까울수록 무채색, 100%에 가까울수록 가장 선명한(순수한)색이 됩니다. OpenCV에서는 0 ~ 255의 범위로 표현됩니다.\n","\n","명도(Value)는 색의 밝고 어두운 정도를 의미합니다. 명도가 높을수록 색상이 밝아지며, 명도가 낮을수록 색상이 어두워집니다.\n","\n","0%에서 100%의 비율로 표현되며, 0%에 가까울수록 검은색, 100%에 가까울수록 가장 맑은색이 됩니다. OpenCV에서는 0 ~ 255의 범위로 표현됩니다.\n","\n","\u003eTip : 0 ~ 360의 범위는 1 Byte(uint8)의 범위를 벗어나게 되므로 불필요한 메모리 사용을 줄이기 위해, 절반의 값인 0 ~ 179의 범위로 표현합니다."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"rqOMRCCn5MqW"},"outputs":[],"source":["import cv2\n","\n","src = cv2.imread(img3,cv2.IMREAD_COLOR)\n","hsv = cv2.cvtColor(src, cv2.COLOR_BGR2HSV)\n","h, s, v = cv2.split(hsv)\n","\n","cv2_imshow(h)\n","cv2_imshow(s)\n","cv2_imshow(v)\n","cv2.waitKey()\n","cv2.destroyAllWindows()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"xQW8QPSA5jfb"},"outputs":[],"source":["import cv2\n","\n","src = cv2.imread(img4, cv2.IMREAD_COLOR)\n","hsv = cv2.cvtColor(src, cv2.COLOR_BGR2HSV)\n","h, s, v = cv2.split(hsv)\n","\n","h = cv2.inRange(h, 8, 20)\n","orange = cv2.bitwise_and(hsv, hsv, mask = h)\n","orange = cv2.cvtColor(orange, cv2.COLOR_HSV2BGR)\n","\n","cv2_imshow(orange)\n","cv2.waitKey()\n","cv2.destroyAllWindows()"]},{"cell_type":"markdown","metadata":{"id":"RjXODqgz5qvo"},"source":["## 배열 병합(addWeighted)\n","영상이나 이미지에서 색상을 검출 할 때, 배열 요소의 범위 설정 함수(cv2.inRange)의 영역이 한정되어 색상을 설정하는 부분이 제한되어 있습니다.\n","\n","예를 들어, 빨간색 영역을 검출하려 할 때, 빨간색 영역이 약 0 ~ 5와 약 170 ~ 179으로 범위가 두 가지로 나눠져 있습니다.\n","\n","이 문제를 해결하려면 배열 요소의 범위 설정 함수를 두 개의 범위로 설정하고 검출한 두 요소의 배열을 병합해서 하나의 공간으로 만들어야 합니다.\n","\n","이때 배열 병합 함수를 사용하며, 서로 다른 두 범위의 배열을 병합할 때 사용합니다.\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"A1y9hT0p6kLH"},"outputs":[],"source":["import cv2\n","\n","src = cv2.imread(img3, cv2.IMREAD_COLOR)\n","hsv = cv2.cvtColor(src, cv2.COLOR_BGR2HSV)\n","h, s, v = cv2.split(hsv)\n","\n","lower_red = cv2.inRange(hsv, (0, 100, 100), (5, 255, 255))\n","upper_red = cv2.inRange(hsv, (170, 100, 100), (180, 255, 255))\n","added_red = cv2.addWeighted(lower_red, 1.0, upper_red, 1.0, 0.0)\n","\n","red = cv2.bitwise_and(hsv, hsv, mask = added_red)\n","red = cv2.cvtColor(red, cv2.COLOR_HSV2BGR)\n","\n","cv2_imshow(red)\n","cv2.waitKey()\n","cv2.destroyAllWindows()"]},{"cell_type":"markdown","metadata":{"id":"SPaDKkp16snU"},"source":["## 채널 분리(Split) 및 병합(Merge)\n","채널 분리(Split)과 병합(Merge)은 영상이나 이미지의 색상 공간의 채널을 분리하거나 합치기 위해 사용합니다.\n","\n","예를 들어, BGR 색상 공간을 B(Blue), G(Green), R(Red)로 분리해 단일 채널을 가진 배열로 반환할 수 있습니다.\n","\n","분리된 채널의 값을 변경하거나 순서를 변경해, GB(R/2) 공간을 만들거나 새로운 색상 공간으로 변경할 수도 있습니다.\n","\n","\u003eTip : OpenCV의 가산 혼합의 삼원색 기본 배열순서는 BGR입니다.\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"B-_WVyu47JTe"},"outputs":[],"source":["import cv2\n","import numpy as np\n","src = cv2.imread(img3, cv2.IMREAD_COLOR)\n","b, g, r = cv2.split(src)\n","inverse = cv2.merge((r, g, b))\n","height, width, channel = src.shape\n","zero = np.zeros((height, width, 1), dtype=np.uint8)\n","bgz = cv2.merge((b, g, zero))\n","\n","cv2_imshow(b)\n","cv2_imshow(g)\n","cv2_imshow(r)\n","cv2_imshow(inverse)\n","cv2_imshow(bgz)\n","cv2.waitKey()\n","cv2.destroyAllWindows()"]},{"cell_type":"markdown","metadata":{"id":"cJiJyU4_pz5n"},"source":["#그리기"]},{"cell_type":"markdown","metadata":{"id":"J8S0wuz78pyf"},"source":["##도형 그리기(Drawing)\n","도형 그리기(Drawing)는 영상이나 이미지 위에 그래픽을 그려 검출 결과를 시각적으로 표시합니다.\n","\n","또한, 이미지 위에 검출 결과를 새롭게 그려 결괏값을 변형하거나 보정하기 위해서도 사용합니다\n","\n","도형 그리기는 직선, 사각형, 원, 다각형 등을 그릴 수 있습니다.\n","\n","도형 그리기는 선형 타입(Line Types), 비트 시프트(Bit Shift)에 따라 결과가 달라질 수 있습니다."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"_1lrDI-z8umu"},"outputs":[],"source":["import cv2\n","import numpy as np\n","\n","src = np.zeros((768, 1366, 3), dtype=np.uint8)\n","\n","src = cv2.line(src, (100, 100), (1200, 100), (0, 0, 255), 3, cv2.LINE_AA)\n","src = cv2.circle(src, (300, 300), 50, (0, 255, 0), cv2.FILLED, cv2.LINE_4)\n","src = cv2.rectangle(src, (500, 200), (1000, 400), (255, 0, 0), 5, cv2.LINE_8)\n","src = cv2.ellipse(src, (1200, 300), (100, 50), 0, 90, 180, (255, 255, 0), 2)\n","\n","pts1 = np.array([[100, 500], [300, 500], [200, 600]])\n","pts2 = np.array([[600, 500], [800, 500], [700, 600]])\n","src = cv2.polylines(src, [pts1], True, (0, 255, 255), 2)\n","src = cv2.fillPoly(src, [pts2], (255, 0, 255), cv2.LINE_AA)\n","\n","src = cv2.putText(src, \"lty\", (900, 600), cv2.FONT_HERSHEY_COMPLEX, 2, (255, 255, 255), 3)\n","\n","cv2_imshow(src)\n","cv2.waitKey()\n","cv2.destroyAllWindows()"]},{"cell_type":"markdown","metadata":{"id":"ErFZ19NHp9rY"},"source":["#형태 변환"]},{"cell_type":"markdown","metadata":{"id":"xg0opZfy87MQ"},"source":["## 기하학적 변환(Geometric Perspective)\n","기하학적 변환(Geometric Transform)이란 이미지를 인위적으로 확대, 축소, 위치 변경, 회전, 왜곡하는 등 이미지의 형태를 변환하는 것을 의미합니다.\n","\n","이미지를 구성하는 픽셀 좌푯값의 위치를 재배치하는 과정으로 볼 수 있습니다.\n","\n","기하학적 변환은 크게 아핀 변환(Affine Transformation)과 원근 변환(Perspective Transformation)이 있다.\n","\n","아핀 변환은 2×3 행렬을 사용하며 행렬 곱셈에 벡터 합을 활용해 표현할 수 있는 변환을 의미합니다.\n","\n","원근 변환은 3×3 행렬을 사용하며, 호모그래피(Homography)로 모델링할 수 있는 변환을 의미합니다.\n","\n","\u003eTip : 아핀 변환은 정확하게는 3×3 행렬 형태를 갖습니다. 행렬의 세 번째 행이 [0, 1, 1] 값을 가져 OpenCV에서는 표현하지 않습니다."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"mmmG7SEb9A68"},"outputs":[],"source":["import cv2\n","import numpy as np\n","\n","src = cv2.imread(img1, cv2.IMREAD_COLOR)\n","height, width, channel = src.shape\n","\n","srcPoint = np.array([[300, 200], [400, 200], [500, 500], [200, 500]], dtype=np.float32)\n","dstPoint = np.array([[0, 0], [width, 0], [width, height], [0, height]], dtype=np.float32)\n","\n","matrix = cv2.getPerspectiveTransform(srcPoint, dstPoint)\n","dst = cv2.warpPerspective(src, matrix, (width, height))\n","\n","cv2_imshow( dst)\n","cv2.waitKey()\n","cv2.destroyAllWindows()"]},{"cell_type":"markdown","metadata":{"id":"5YN_9g4CqOSE"},"source":["#검출"]},{"cell_type":"markdown","metadata":{"id":"ec58B-JYqRZl"},"source":["##윤곽선(Contour)\n","영상이나 이미지의 윤곽선(컨투어)을 검출하기 위해 사용합니다.\n","\n","영상이나 이미지에서 외곽과 내곽의 윤곽선(컨투어)을 검출할 수 있습니다."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"WS0HahGmsyw-"},"outputs":[],"source":["import cv2\n","\n","src = cv2.imread(img4, cv2.IMREAD_COLOR)\n","\n","gray = cv2.cvtColor(src, cv2.COLOR_RGB2GRAY)\n","ret, binary = cv2.threshold(gray, 127, 255, cv2.THRESH_BINARY)\n","binary = cv2.bitwise_not(binary)\n","\n","contours, hierarchy = cv2.findContours(binary, cv2.RETR_CCOMP, cv2.CHAIN_APPROX_NONE)\n","\n","for i in range(len(contours)):\n","    cv2.drawContours(src, [contours[i]], 0, (0, 0, 255), 2)\n","    cv2.putText(src, str(i), tuple(contours[i][0][0]), cv2.FONT_HERSHEY_COMPLEX, 0.8, (0, 255, 0), 1)\n","    print(i, hierarchy[0][i])\n","    cv2_imshow(src)\n","    cv2.waitKey(0)\n","\n","cv2.destroyAllWindows()"]},{"cell_type":"markdown","metadata":{"id":"nbbyjoUXqRSF"},"source":["##다각형 근사(Approx Poly)\n","영상이나 이미지의 윤곽점을 압축해 다각형으로 근사하기 위해 사용합니다.\n","\n","영상이나 이미지에서 윤곽선의 근사 다각형을 검출할 수 있습니다."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"_1Pn2BC5s60C"},"outputs":[],"source":["import cv2\n","\n","src = cv2.imread(img4, cv2.IMREAD_COLOR)\n","\n","gray = cv2.cvtColor(src, cv2.COLOR_RGB2GRAY)\n","ret, binary = cv2.threshold(gray, 0, 255, cv2.THRESH_OTSU)\n","binary = cv2.bitwise_not(binary)\n","\n","contours, hierarchy = cv2.findContours(binary, cv2.RETR_LIST, cv2.CHAIN_APPROX_TC89_KCOS)\n","\n","for contour in contours:\n","    epsilon = cv2.arcLength(contour, True) * 0.02\n","    approx_poly = cv2.approxPolyDP(contour, epsilon, True)\n","\n","    for approx in approx_poly:\n","        cv2.circle(src, tuple(approx[0]), 3, (255, 0, 0), -1)\n","\n","cv2_imshow(src)\n","cv2.waitKey(0)\n","cv2.destroyAllWindows()"]},{"cell_type":"markdown","metadata":{"id":"bZDVYN84qRD4"},"source":["##블록 껍질(Convex Hull)\n","윤곽선(points, contours)의 경계면을 둘러싸는 다각형을 구하는 알고리즘입니다.\n","\n","반환되는 결과는 윤곽선 검출 결과와 동일한 형식을 띄며, 스크랜스키(Sklansky) 알고리즘을 이용해 입력된 좌표들의 볼록한 외곽을 찾습니다.\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"bfWw05f_tLrh"},"outputs":[],"source":["import cv2\n","\n","src = cv2.imread(img4)\n","dst = src.copy()\n","\n","gray = cv2.cvtColor(src, cv2.COLOR_RGB2GRAY)\n","ret, binary = cv2.threshold(gray, 150, 255, cv2.THRESH_BINARY_INV)\n","\n","contours, hierarchy = cv2.findContours(binary, cv2.RETR_CCOMP, cv2.CHAIN_APPROX_NONE)\n","\n","for i in contours:\n","    hull = cv2.convexHull(i, clockwise=True)\n","    cv2.drawContours(dst, [hull], 0, (0, 0, 255), 2)\n","\n","cv2_imshow(dst)\n","cv2.waitKey(0)\n","cv2.destroyAllWindows()"]},{"cell_type":"markdown","metadata":{"id":"B0Q5mi6jqQ00"},"source":["##모멘트(Moments)\n","윤곽선(contour)이나 이미지(array)의 0차 모멘트부터 3차 모멘트까지 계산하는 알고리즘입니다.\n","\n","공간 모멘트(spatial moments), 중심 모멘트(central moments), 정규화된 중심 모멘트(normalized central moments), 질량 중심(mass center) 등을 계산할 수 있습니다.\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"OGBE66sJtW9V"},"outputs":[],"source":["import cv2\n","\n","src = cv2.imread(img4)\n","dst = src.copy()\n","\n","gray = cv2.cvtColor(src, cv2.COLOR_RGB2GRAY)\n","ret, binary = cv2.threshold(gray, 150, 255, cv2.THRESH_BINARY_INV)\n","\n","contours, hierarchy = cv2.findContours(binary, cv2.RETR_CCOMP, cv2.CHAIN_APPROX_NONE)\n","\n","for i in contours:\n","    M = cv2.moments(i)\n","    cX = int(M['m10'] / M['m00'])\n","    cY = int(M['m01'] / M['m00'])\n","\n","    cv2.circle(dst, (cX, cY), 3, (255, 0, 0), -1)\n","    cv2.drawContours(dst, [i], 0, (0, 0, 255), 2)\n","\n","cv2_imshow(dst)\n","cv2.waitKey(0)\n","cv2.destroyAllWindows()"]},{"cell_type":"markdown","metadata":{"id":"48302xsakpMF"},"source":["## 직선 검출(Line Detection)\n","\n","\n","직선 검출 알고리즘은 허프 변환(Hough Transform)을 활용해 직선을 검출합니다.\n","\n","허프 변환은 이미지에서 직선을 찾는 가장 보편적인 알고리즘입니다.\n","\n","이미지에서 선과 같은 단순한 형태를 빠르게 검출할 수 있으며, 직선을 찾아 이미지나 영상을 보정하거나 복원합니다.\n","\n","허프 선 변환은 이미지 내의 어떤 점이라도 선 집합의 일부일 수 있다는 가정하에 직선의 방정식을 이용해 직선을 검출한다.\n","\n","$$직선 검출은 직선의 방정식을 활용해 y=ax+b를 극좌표(ρ, θ)의 점으로 변환해서 사용합니다.$$\n","\n","$$극좌표 방정식으로 변환한다면 p=x sinθ+y cosθ이 되어, 직선과 원점의 거리(ρ)와 직선과 x축이 이루는 각도(θ)를 구할 수 있습니다.$$\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"TlHhGOYrluTo"},"outputs":[],"source":["import numpy as np\n","import cv2\n","\n","src = cv2.imread(img3)\n","dst = src.copy()\n","gray = cv2.cvtColor(src, cv2.COLOR_BGR2GRAY)\n","canny = cv2.Canny(gray, 5000, 1500, apertureSize = 5, L2gradient = True)\n","lines = cv2.HoughLinesP(canny, 0.8, np.pi / 180, 90, minLineLength = 10, maxLineGap = 100)\n","\n","for i in lines:\n","    cv2.line(dst, (int(i[0][0]), int(i[0][1])), (int(i[0][2]), int(i[0][3])), (0, 0, 255), 2)\n","\n","cv2_imshow(dst)\n","cv2.waitKey(0)\n","cv2.destroyAllWindows()"]},{"cell_type":"markdown","metadata":{"id":"37L62ySDjd-o"},"source":["#모폴로지"]},{"cell_type":"markdown","metadata":{"id":"fodQHVJmjgvV"},"source":["##모폴로지 변환(Morphological Transformation)\n","모폴로지 변환(Perspective Transformation)은 영상이나 이미지를 형태학적 관점에서 접근하는 기법을 의미합니다.\n","\n","모폴로지 변환은 주로 영상 내 픽셀값 대체에 사용됩니다. 이를 응용해서 노이즈 제거, 요소 결합 및 분리, 강도 피크 검출 등에 이용할 수 있습니다.\n","\n","집합의 포함 관계, 이동(translation), 대칭(reflection), 여집합(complement), 차집합(difference) 등의 성질을 사용합니다.\n","\n","기본적인 모폴로지 변환으로는 팽창(dilation)과 침식(erosion)이 있습니다.\n","\n","팽창과 침식은 이미지와 커널의 컨벌루션 연산이며, 이 두 가지 기본 연산을 기반으로 복잡하고 다양한 모폴로지 연산을 구현할 수 있습니다."]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"2pBS8v-GkI3H"},"outputs":[],"source":["import numpy as np\n","import cv2\n","\n","src = cv2.imread(img3)\n","\n","kernel = cv2.getStructuringElement(cv2.MORPH_CROSS, (9, 9))\n","dilate = cv2.dilate(src, kernel, anchor=(-1, -1), iterations=5)\n","erode = cv2.erode(src, kernel, anchor=(-1, -1), iterations=5)\n","\n","dst = np.concatenate((src, dilate, erode), axis=1)\n","\n","cv2_imshow(dst)\n","cv2.waitKey(0)\n","cv2.destroyAllWindows()"]},{"cell_type":"markdown","metadata":{"id":"9YP-N7rHj7Q0"},"source":["## 모폴로지 연산(Morphological Calculate)Permalink\n","모폴로지 연산(Perspective Calculate)은 모폴로지 변환의 팽창(dilation)과 침식(erosion)을 기본 연산으로 사용해 고급 형태학을 적용하는 변환 연산입니다.\n","\n","입력 이미지가 이진화된 이미지라면 팽창과 침식 연산으로도 우수한 결과를 얻을 수 있습니다.\n","\n","하지만, 그레이스케일이나 다중 채널 이미지를 사용하는 경우 더 복잡한 연산을 필요로 합니다.\n","\n","이때 모폴로지 연산을 활용해 우수한 결과를 얻을 수 있습니다."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Dp5WmANMkcvw"},"outputs":[],"source":["import numpy as np\n","import cv2\n","\n","src = cv2.imread(img3)\n","\n","kernel = cv2.getStructuringElement(cv2.MORPH_CROSS, (9, 9))\n","dst = cv2.morphologyEx(src, cv2.MORPH_OPEN, kernel, iterations=9)\n","\n","cv2_imshow(dst)\n","cv2.waitKey(0)\n","cv2.destroyAllWindows()"]},{"cell_type":"markdown","metadata":{"id":"YBg7KcFjkxC2"},"source":["#연산"]},{"cell_type":"markdown","metadata":{"id":"p07vvAU6pW9F"},"source":["##이미지 연산(Image Calculation)\n","이미지 연산은 하나 또는 둘 이상의 이미지에 대해 수학적인 연산을 수행합니다.\n","\n","Numpy 클래스의 배열 연산과 동일하거나 비슷한 의미와 결과를 갖습니다.\n","\n","또한, 대수적 표현(+, - 등)을 통해 Mat 클래스 간의 연산을 수행할 수 있습니다."]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"xLEZgT6fpzAK"},"outputs":[],"source":["import numpy as np\n","import cv2\n","\n","src = cv2.imread(img3)\n","number = np.ones_like(src) * 127\n","\n","_max = cv2.max(src, number)\n","_min = cv2.min(src, number)\n","_abs = cv2.absdiff(src, number)\n","compare = cv2.compare(src, number, cv2.CMP_GT)\n","\n","src = np.concatenate((src, src, src, src), axis = 1)\n","number = np.concatenate((number, number, number, number), axis = 1)\n","dst = np.concatenate((_max, _min, _abs, compare), axis = 1)\n","\n","dst = np.concatenate((src, number, dst), axis = 0)\n","\n","cv2_imshow(dst)\n","cv2.waitKey(0)\n","cv2.destroyAllWindows()"]},{"cell_type":"markdown","metadata":{"id":"PPP8yf5zqHfM"},"source":["##비트 연산(Bitwise)\n","비트 연산은 하나 또는 두 이미지에 대해 비트 연산을 수행합니다.\n","\n","Numpy 클래스의 비트 연산과 동일한 의미와 결과를 갖습니다.\n","\n","또한, 비트 연산 표현(\u0026, | 등)을 통해 Mat 클래스 간의 연산을 수행할 수 있습니다."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"X63m4-xrqN6u"},"outputs":[],"source":["import numpy as np\n","import cv2\n","\n","src = cv2.imread(img1)\n","gray = cv2.cvtColor(src, cv2.COLOR_BGR2GRAY)\n","_, binary = cv2.threshold(gray, 127, 255, cv2.THRESH_BINARY)\n","\n","_and = cv2.bitwise_and(gray, binary)\n","_or = cv2.bitwise_or(gray, binary)\n","_xor = cv2.bitwise_xor(gray, binary)\n","_not = cv2.bitwise_not(gray)\n","\n","src = np.concatenate((np.zeros_like(gray), gray, binary, np.zeros_like(gray)), axis = 1)\n","dst = np.concatenate((_and, _or, _xor, _not), axis = 1)\n","dst = np.concatenate((src, dst), axis = 0)\n","\n","cv2_imshow(dst)\n","cv2.waitKey(0)\n","cv2.destroyAllWindows()\n"]},{"cell_type":"markdown","metadata":{"id":"oPsfUAcxqf5O"},"source":["##히스토그램(Histogram)\n","\n","히스토그램이란 도수 분포표 중 하나로 데이터의 분포를 몇 개의 구간으로 나누고 각 구간에 속하는 데이터를 시각적으로 표현한 막대그래프입니다.\n","\n","이미지에서 사용하는 히스토그램은 X 축을 픽셀의 값으로 사용하고 Y 축을 해당 픽셀의 개수로 표현합니다.\n","\n","이미지의 픽셀값을 히스토그램으로 표현하면 이미지의 특성을 쉽게 확인할 수 있습니다.\n","\n","히스토그램은 다음과 같은 세 가지의 중요한 요소를 갖고 있습니다.\n","\n","\n","\n","1.    빈도 수(BINS): 히스토그램 그래프의 X 축 간격\n","2.    차원 수(DIMS): 히스토그램을 분석할 이미지의 차원\n","3.    범위(RANGE): 히스토그램 그래프의 X 축 범위\n","\n","\n","빈도 수는 히스토그램의 X 축 간격입니다. 픽셀값의 범위는 0~255로 총 256개의 범위를 갖고 있으며, 빈도 수의 값이 8이라면 0 ~ 7, 8 ~ 15, …, 248 ~ 255의 범위로 총 32개의 막대가 생성됩니다.\n","\n","차원 수는 이미지에서 분석하고자 하는 색상 차원을 의미합니다. 그레이스케일은 단일 채널이므로 하나의 차원에 대해 분석할 수 있고 색상 이미지는 다중 채널이므로 세 개 이상의 차원에 대해 분석할 수 있습니다.\n","\n","범위는 이미지에서 측정하려는 픽셀값의 범위로서, 특정 픽셀값 영역에 대해서만 분석하게 하는 데 사용됩니다."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"8cgDz-fgqxSC"},"outputs":[],"source":["import cv2\n","import numpy as np\n","\n","src = cv2.imread(img1)\n","gray = cv2.cvtColor(src, cv2.COLOR_BGR2GRAY)\n","result = np.zeros((src.shape[0], 256), dtype=np.uint8)\n","\n","hist = cv2.calcHist([gray], [0], None, [256], [0, 256])\n","cv2.normalize(hist, hist, 0, result.shape[0], cv2.NORM_MINMAX)\n","\n","for x, y in enumerate(hist):\n","    cv2.line(result, (int(x), result.shape[0]), (int(x), result.shape[0] - int(y)), 255)\n","\n","dst = np.hstack([gray, result])\n","\n","cv2_imshow(dst)\n","cv2.waitKey(0)\n","cv2.destroyAllWindows()"]},{"cell_type":"markdown","metadata":{"id":"AAKkEmX-rBM5"},"source":["## 픽셀 접근(Pixel Access)\n","픽셀 접근은 이미지 배열에서 특정 좌표에 대한 값을 받아오거나, 변경할 때 사용합니다.\n","\n","Numpy 배열의 요소 접근 방식과 동일하며, 직접 값을 변경하거나 할당할 수 있습니다.\n","\n","OpenCV의 Mat 클래스는 Numpy 배열을 사용하므로 문자열, 리스트, 튜플 등에 사용되는 슬라이싱을 동일하게 사용할 수 있습니다."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"2zliyM0_rFdx"},"outputs":[],"source":["import cv2\n","import numpy as np\n","\n","gray = np.linspace(0, 255, num=90000, endpoint=True, retstep=False, dtype=np.uint8).reshape(300, 300, 1)\n","color = np.zeros((300, 300, 3), np.uint8)\n","color[0:150, :, 0] = gray[0:150, :, 0]\n","color[:, 150:300, 2] = gray[:, 150:300, 0]\n","\n","x, y, c = 200, 100, 0\n","access_gray = gray[y, x, c]\n","access_color_blue = color[y, x, c]\n","access_color = color[y, x]\n","\n","print(access_gray)\n","print(access_color_blue)\n","print(access_color)\n","\n","cv2_imshow(gray)\n","cv2_imshow(color)\n","cv2.waitKey(0)\n","cv2.destroyAllWindows()"]},{"cell_type":"markdown","metadata":{"id":"aFG3EdLRrWy8"},"source":["## 적응형 이진화(Adaptive Threshold)\n","적응형 이진화 알고리즘은 입력 이미지에 따라 임곗값이 스스로 다른 값을 할당할 수 있도록 구성된 이진화 알고리즘입니다.\n","\n","이미지에 따라 어떠한 임곗값을 주더라도 이진화 처리가 어려운 이미지가 존재합니다.\n","\n","예를 들어, 조명의 변화나 반사가 심한 경우 이미지 내의 밝기 분포가 달라 국소적으로 임곗값을 적용해야 하는 경우가 있습니다.\n","\n","이러한 경우 적응형 이진화 알고리즘을 적용한다면 우수한 결과를 얻을 수 있습니다.\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"LoUsfZW3rby4"},"outputs":[],"source":["import cv2\n","\n","src = cv2.imread(img1)\n","gray = cv2.cvtColor(src, cv2.COLOR_BGR2GRAY)\n","binary = cv2.adaptiveThreshold(gray, 255, cv2.ADAPTIVE_THRESH_MEAN_C, cv2.THRESH_BINARY, 467, 37)\n","\n","cv2_imshow(binary)\n","cv2.waitKey(0)\n","cv2.destroyAllWindows()"]},{"cell_type":"markdown","metadata":{"id":"ksPhXebLs-Pe"},"source":["#찾기"]},{"cell_type":"markdown","metadata":{"id":"tJWIPCHfsNTE"},"source":["##템플릿 매칭(Template Matching)\n","템플릿 매칭은 원본 이미지에서 템플릿 이미지와 일치하는 영역을 찾는 알고리즘입니다.\n","\n","원본 이미지 위에 템플릿 이미지를 놓고 조금씩 이동해가며 이미지 끝에 도달할 때 까지 비교해 찾아갑니다.\n","\n","이 방식을 통해, 템플릿 이미지와 동일하거나, 가장 유사한 영역을 원본 이미지에서 검출합니다.\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"EttI7SmwsASb"},"outputs":[],"source":["import cv2\n","\n","src = cv2.imread(img5, cv2.IMREAD_GRAYSCALE)\n","templit = cv2.imread(img6, cv2.IMREAD_GRAYSCALE)\n","dst = cv2.imread(img5)\n","\n","result = cv2.matchTemplate(src, templit, cv2.TM_SQDIFF_NORMED)\n","\n","minVal, maxVal, minLoc, maxLoc = cv2.minMaxLoc(result)\n","x, y = minLoc\n","h, w = templit.shape\n","\n","dst = cv2.rectangle(dst, (x, y), (x +  w, y + h) , (0, 0, 255), 1)\n","cv2_imshow(dst)\n","cv2.waitKey(0)\n","cv2.destroyAllWindows()"]},{"cell_type":"markdown","metadata":{"id":"CmcM5lpcsra_"},"source":["## ORB(Oriented FAST and Rotated BRIEF)\n","ORB 알고리즘은 FAST 알고리즘을 사용해 특징점을 검출합니다.\n","\n","FAST 알고리즘은 코너뿐만 아니라 가장자리에도 반응하는 문제점으로 인해 해리스 코너 검출 알고리즘을 적용해 최상위 특징점만 추출합니다.\n","\n","이 과정에서 이미지 피라미드를 구성해 스케일 공간 검색을 수행합니다.\n","\n","이후 스케일 크기에 따라 피처 주변 박스 안의 강도 분포에 대해 X축과 Y축을 기준으로 1차 모멘트를 계산합니다.\n","\n","1차 모멘트는 그레이디언트의 방향을 제공하므로 피처의 방향을 지정할 수 있습니다.\n","\n","방향이 지정되면 해당 방향에 대해 피처 벡터를 계산할 수 있으며, 피처는 회전 불변성을 갖고 있으며 방향 정보를 포함하고 있습니다.\n","\n","하나의 ORB 피처를 가져와 피처 주변의 박스에서 1차 모멘트와 방위 벡터를 계산합니다.\n","\n","피처의 중심에서 모멘트가 가리키는 위치까지 벡터를 피처 방향으로 부여하게 됩니다. ORB의 기술자는 BRIEF 기술자에 없는 방향 정보를 갖고 있습니다.\n","\n","ORB 알고리즘은 SIFT(Scale-Invariant Feature Trasnform) 알고리즘과 SURF(Speeded-Up Robust Features) 알고리즘 을 대체하기 위해 OpenCV Labs에서 개발됐으며 속도 또한 더 빨라졌습니다.\n","\n","\u003eTip : 회전 불변성이란 이미지가 회전돼 있어도 기술자는 회전 전과 같은 값으로 계산됩니다. 회전 불변성을 갖고 있지 않다면 회전된 이미지에서 피처는 서로 다른 의미(값)를 지니게 됩니다.\n","\n","\u003eTip : OpenCV 4 부터는 SIFT 알고리즘과 SURF 알고리즘을 지원하지 않습니다."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Tqf1P4XNtfNU"},"outputs":[],"source":["import cv2\n","import numpy as np\n","\n","src = cv2.imread(img5)\n","gray = cv2.cvtColor(src, cv2.COLOR_BGR2GRAY)\n","target = cv2.imread(img6, cv2.IMREAD_GRAYSCALE)\n","\n","orb = cv2.ORB_create(\n","    nfeatures=40000,\n","    scaleFactor=1.2,\n","    nlevels=8,\n","    edgeThreshold=31,\n","    firstLevel=0,\n","    WTA_K=2,\n","    scoreType=cv2.ORB_HARRIS_SCORE,\n","    patchSize=31,\n","    fastThreshold=20,\n",")\n","\n","kp1, des1 = orb.detectAndCompute(gray, None)\n","kp2, des2 = orb.detectAndCompute(target, None)\n","\n","bf = cv2.BFMatcher(cv2.NORM_HAMMING, crossCheck=True)\n","matches = bf.match(des1, des2)\n","matches = sorted(matches, key=lambda x: x.distance)\n","\n","for i in matches[:100]:\n","    idx = i.queryIdx\n","    x1, y1 = kp1[idx].pt\n","    cv2.circle(src, (int(x1), int(y1)), 3, (255, 0, 0), 3)\n","\n","cv2_imshow(src)\n","cv2.waitKey()"]},{"cell_type":"markdown","metadata":{"id":"7QMRtlWKwxIh"},"source":["#변환"]},{"cell_type":"markdown","metadata":{"id":"jhd_suSJwTOA"},"source":["##리매핑(Remapping)\n","\n","리매핑(Remapping)은 입력 이미지에 기하학적인 변형을 적용하는 방법입니다.\n","\n","기하학적 변환에서 다루었던 아핀 변환(Affine Transform)과 원근 변환(Perspective Transform)은 이미지에 변환 행렬을 적용하여, 이미지를 변경합니다.\n","\n","리매핑은 이미지에 변환 행렬 연산을 적용하는 것이 아닌, 비선형 변환을 적용할 수 있습니다.\n","\n","즉, 픽셀들의 좌표를 임의의 특정 좌표로 옮겨 이미지를 변경하는 작업을 의미합니다.\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"_oKHbE7EwZZj"},"outputs":[],"source":["import cv2\n","import numpy as np\n","\n","src = cv2.imread(img1)\n","height, width = src.shape[:2]\n","map2, map1 = np.indices((height, width), dtype=np.float32)\n","\n","map1 = map1 + width / 100 * np.sin(map1)\n","map2 = map2 + height / 100 * np.cos(map2)\n","\n","dst = cv2.remap(src, map1, map2, cv2.INTER_CUBIC)\n","cv2_imshow(dst)\n","cv2.waitKey()\n"]},{"cell_type":"markdown","metadata":{"id":"ONx8PadEw4kW"},"source":["## 색상 맵(Color Map)\n","색상 맵(Color Map)은 입력 이미지에 순람표(Lookup table) 구조로 이루어진 데이터를 적용합니다.\n","\n","주로 데이터를 시각화하기 위해 사용되며, 색상의 분포표로 데이터를 쉽게 확인할 수 있습니다.\n","\n","픽셀값이 1:1로 매칭되기 때문에 선형 구조나 비선형 구조로도 데이터를 매핑해 표현할 수 있습니다.\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"X72dVFKtw8x4"},"outputs":[],"source":["import cv2\n","\n","src = cv2.imread(img1)\n","dst = cv2.applyColorMap(src, cv2.COLORMAP_OCEAN)\n","\n","cv2_imshow(dst)\n","cv2.waitKey()\n","cv2.destroyAllWindows()"]},{"cell_type":"markdown","metadata":{"id":"6GbULJVwxH77"},"source":["#k-알고리즘"]},{"cell_type":"markdown","metadata":{"id":"O9OtXiZWxNn-"},"source":["## K-평균 군집화 알고리즘(K-means Clustering Algorithm)\n","K-평균 군집화 알고리즘(K-means Clustering Algorithm)은 비지도 학습의 대표적인 알고리즘 중 하나로 라벨(Label)이 달려 있지 않은 입력 데이터에 라벨을 달아줍니다.\n","\n","K-평균 군집화 알고리즘의 방식은 임의의 K개의 중심점(Centroid)를 기준으로 최소 거리에 기반한 군집화를 진행합니다.\n","\n","각각의 데이터는 가장 가까운 중심에 군집(Cluster)을 이루며, 같은 중심에 할당된 데이터는 하나의 군집군으로 형성됩니다."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"5X3Y13MqxUNn"},"outputs":[],"source":["import numpy as np\n","import cv2\n","\n","src = cv2.imread(img1)\n","\n","data = src.reshape(-1, 3).astype(np.float32)\n","criteria = (cv2.TERM_CRITERIA_MAX_ITER + cv2.TERM_CRITERIA_EPS, 10, 0.001)\n","retval, bestLabels, centers = cv2.kmeans(data, 5, None, criteria, 10, cv2.KMEANS_RANDOM_CENTERS)\n","\n","centers = centers.astype(np.uint8)\n","dst = centers[bestLabels].reshape(src.shape)\n","\n","cv2_imshow(dst)\n","cv2.waitKey()\n","cv2.destroyAllWindows()"]},{"cell_type":"markdown","metadata":{"id":"J_e2ec47xgw9"},"source":["## K-최근접 이웃(K-Nearest Neighbor Algorithm)\n","\n","\n","K-최근접 이웃 알고리즘(K-Nearest Neighbor Algorithm, KNN)은 지도 학습에 사용할 수 있는 가장 간단한 분류 알고리즘 중 하나로, 회귀 분석이나 분류에서 사용되는 알고리즘입니다.\n","\n","이 알고리즘은 새로운 데이터가 입력되었을 때 기존의 데이터와 가장 가까운 k개 데이터의 정보로 새로운 데이터를 예측하는 방법입니다.\n","\n","즉, 새로운 데이터 주변에 분포해 있는 이웃 데이터의 성질을 토대로 판단합니다.\n","\n","\u003cimg src='https://076923.github.io/assets/posts/Python/OpenCV/lecture-43/1.webp'\u003e\n","\n","위 그림에는 파란색, 초록색, 빨간색의 원이 존재합니다.\n","\n","이 그림에는 파란색과 빨간색의 두 그룹이 존재합니다. 이 그룹군들을 클래스(Class)라고 부르며, N차원 특징 공간(Feature Space)에 표현될 수 있습니다.\n","\n","이 특징 공간에 새로운 데이터인 초록색 원이 주어졌을 때, K-최근접 이웃 알고리즘은 어느 클래스에 분류될지를 판단하게 됩니다.\n","\n","이때 최근접 이웃(Nearest Neighbor) 방식을 사용하며, 가장 가까운 지점간의 거리가 가장 짧은 클래스로 분류됩니다.\n","\n","만약, K=1이라면 초록색 원에서 가장 가까운 1개의 원은 파란색 원이 되어 파란색 클래스가 되고, K=7이라면 가장 가까운 원은 파란색 원 3개, 빨간색 원 4개로 초록색 원은 빨간색 클래스가 됩니다.\n","\n","즉, K-최근접 이웃 알고리즘은 새로운 데이터가 입력되었을 때 가장 가까운 K개를 비교하여 가장 거리가 가까운 개수가 많은 클래스로 분류됩니다.\n","\n","여기서 주의해야할 사항으로는 K가 짝수라면 근접한 클래스의 개수가 동점이 발생할 수 있습니다.\n","\n","K-최근접 이웃 알고리즘은 동점이라도 거리가 더 가까이에 있는 클래스에 가중치를 부여하지 않으므로, 가능한 K를 홀수로 사용하는 것이 좋습니다."]},{"cell_type":"markdown","metadata":{"id":"fdHNPTCeynMw"},"source":["\n","\n","```\n","import cv2\n","import numpy as np\n","\n","\n","def loadTrainData(image_path, label_path):\n","    with open(image_path, \"rb\") as image_data:\n","        images = np.frombuffer(image_data.read(), dtype=np.uint8, offset=16)\n","    with open(label_path, \"rb\") as label_data:\n","        labels = np.frombuffer(label_data.read(), dtype=np.uint8, offset=8)\n","    return images.reshape(-1, 784), labels\n","\n","\n","train_x, train_y = loadTrainData(\n","    \"./fashion-mnist/train-images-idx3-ubyte\",\n","    \"./fashion-mnist/train-labels-idx1-ubyte\"\n",")\n","test_x, test_y = loadTrainData(\n","    \"./fashion-mnist/t10k-images-idx3-ubyte\",\n","    \"./fashion-mnist/t10k-labels-idx1-ubyte\"\n",")\n","\n","label_dict = {\n","    0: \"T-shirt/top\",\n","    1: \"Trouser\",\n","    2: \"Pullover\",\n","    3: \"Dress\",\n","    4: \"Coat\",\n","    5: \"Sandal\",\n","    6: \"Shirt\",\n","    7: \"Sneaker\",\n","    8: \"Bag\",\n","    9: \"Ankle boot\",\n","}\n","\n","knn = cv2.ml.KNearest_create()\n","retval = knn.train(train_x.astype(np.float32), cv2.ml.ROW_SAMPLE, train_y.astype(np.int32))\n","\n","count = 500\n","retval, results, neighborResponses, dist = knn.findNearest(\n","    test_x[:count].astype(np.float32), k=7\n",")\n","\n","matches = results.astype(np.uint8) == test_y[:count][:, None]\n","print(np.count_nonzero(matches) / count * 100)\n","\n","for idx, result in enumerate(results):\n","    print(\"Index : {}\".format(idx))\n","    print(\"예측값 : {}\".format(label_dict[int(result)]))\n","    print(\"실제값 : {}\".format(label_dict[test_y[idx]]))\n","    cv2_imshow(test_x[idx].reshape(28, 28, 1))\n","    cv2.waitKey()\n","```\n","\n"]}],"metadata":{"colab":{"authorship_tag":"ABX9TyM0l8F0fgp/My2f7odL0g40","name":"","toc_visible":true,"version":""},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}